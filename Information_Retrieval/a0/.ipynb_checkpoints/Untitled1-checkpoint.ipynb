{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\" Convert a string representing one document into a list of\n",
    "    words. Remove all punctuation and split on whitespace.\n",
    "    Here is a doctest: \n",
    "    >>> tokenize(\"Hi  there. What's going on?\")\n",
    "    ['hi', 'there', 'what', 's', 'going', 'on']\n",
    "    \"\"\"\n",
    "    temp_list1 = []\n",
    "    temp_1 = re.sub(r'[^\\w\\s]',' ',document).lower()\n",
    "    temp_list1 = temp_1.split()\n",
    "    \n",
    "    return temp_list1\n",
    "    ###TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'there', 'what', 's', 'going', 'on']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Hi  there. What's going on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_index(tokens):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a list of document tokens. The index maps\n",
    "    each unique word to a list of document ids, sorted in increasing order.\n",
    "    Params:\n",
    "      tokens...A list of lists of strings\n",
    "    Returns:\n",
    "      An inverted index. This is a dict where keys are words and values are\n",
    "      lists of document indices, sorted in increasing order.\n",
    "    Below is an example, where the first document contains the tokens 'a' and\n",
    "    'b', and the second document contains the tokens 'a' and 'c'.\n",
    "    >>> index = create_index([['a', 'b'], ['a', 'c']])\n",
    "    >>> sorted(index.keys())\n",
    "    ['a', 'b', 'c']\n",
    "    >>> index['a']\n",
    "    [0, 1]\n",
    "    >>> index['b']\n",
    "    [0]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    index = {}\n",
    "    \n",
    "    for each_list in tokens:\n",
    "        for each_element in each_list:\n",
    "            if each_element not in index:\n",
    "                index[each_element] = []\n",
    "                index[each_element].append(tokens.index(each_list))\n",
    "            else:\n",
    "                index[each_element].append(tokens.index(each_list))\n",
    "                \n",
    "    for i in index:\n",
    "        index[i] = sorted(list(set(index[i])))\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [0, 1], 'b': [0], 'c': [1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_index([['a', 'b', 'b'], ['a', 'c']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def intersect(list1, list2):\n",
    "    \"\"\" Return the intersection of two posting lists. Use the optimize\n",
    "    algorithm of Figure 1.6 of the MRS text. Your implementation should be\n",
    "    linear in the sizes of list1 and list2. That is, you should only loop once\n",
    "    through each list.\n",
    "    Params:\n",
    "      list1....A list of document indices, sorted in ascending order.\n",
    "      list2....Another list of document indices, sorted in ascending order.\n",
    "    Returns:\n",
    "      The list of document ids that appear in both lists, sorted in ascending order.\n",
    "    >>> C\n",
    "    [3, 5]\n",
    "    >>> intersect([1, 2], [3, 4])\n",
    "    []\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    i = 0\n",
    "    j = 0\n",
    "    temp_list2 = []\n",
    "    while i < len(list1) and j < (len(list2)):\n",
    "            if list1[i] == list2[j]:\n",
    "                temp_list2.append(list1[i])\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            elif list1[i] < list2[j]:\n",
    "                i = i + 1\n",
    "            elif list2[j] < list1[i]:\n",
    "                j = j + 1\n",
    "    return temp_list2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect([0, 7 , 8 ], [0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect([1, 3, 5], [3, 4, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sort_by_num_postings(words, index):\n",
    "    \"\"\"\n",
    "    Sort the words in increasing order of the length of their postings list in\n",
    "    index. You may use Python's builtin sorted method.\n",
    "    Params:\n",
    "      words....a list of strings.\n",
    "      index....An inverted index; a dict mapping words to lists of document\n",
    "      ids, sorted in ascending order.\n",
    "    Returns:\n",
    "      A list of words, sorted in ascending order by the number of document ids\n",
    "      in the index.\n",
    "\n",
    "    >>> sort_by_num_postings(['a', 'b', 'c'], {'a': [0, 1], 'b': [1, 2, 3], 'c': [4]})\n",
    "    ['c', 'a', 'b']\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    dict_1 = {}\n",
    "    for i in words:\n",
    "        dict_1[i] = len(index[i])\n",
    "    return [x[0] for x in sorted(dict_1.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'a', 'c']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_by_num_postings(['a', 'b', 'c'], {'a': [0, 1, 5, 7], 'b': [1, 2, 3], 'c': [4, 10, 40, 55, 65, 92]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(index, query):\n",
    "    \"\"\" Return the document ids for documents matching the query. Assume that\n",
    "    query is a single string, possibly containing multiple words. The steps\n",
    "    are to:\n",
    "    1. tokenize the query\n",
    "    2. Sort the query words by the length of their postings list\n",
    "    3. Intersect the postings list of each word in the query.\n",
    "\n",
    "    If a query term is not in the index, then an empty list should be returned.\n",
    "\n",
    "    Params:\n",
    "      index...An inverted index (dict mapping words to document ids)\n",
    "      query...A string that may contain multiple search terms. We assume the\n",
    "      query is the AND of those terms by default.\n",
    "\n",
    "    E.g., below we search for documents containing 'a' and 'b':\n",
    "    >>> search({'a': [0, 1], 'b': [1, 2, 3], 'c': [4]}, 'a b')\n",
    "    [1]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    temp_list = []\n",
    "    tok = tokenize(query)\n",
    "    io = sort_by_num_postings(tok, index)\n",
    "    temp_list = index[io[0]]\n",
    "    \n",
    "    for i in io:\n",
    "            temp_list = intersect(temp_list, index[i])\n",
    "    \n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search({'a': [0, 1,2], 'b': [1, 2, 3], 'c': [1,2, 4, 3]}, 'a b c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "QUERY:lion\n",
      "\n",
      "RESULTS:\n",
      "How do you keep a lion from charging?     Take away its credit cards.\n",
      "\n",
      "\n",
      "\n",
      "QUERY:What has\n",
      "\n",
      "RESULTS:\n",
      "What has four legs and goes booo?     A cow with a cold\n",
      "\n",
      "What has 10 letters and starts with gas?     An automobile.\n",
      "\n",
      "\n",
      "\n",
      "QUERY:why because\n",
      "\n",
      "RESULTS:\n",
      "Why did the elephant decide not to move?     Because he couldn't lift his trunk.\n",
      "\n",
      "Why did the strawberry cross the road?     Because his mother was in a jam.\n",
      "\n",
      "Why is the little duck always so sad?     Because he always sees a bill in front of his face.\n",
      "\n",
      "Why did they bury the battery?     Because it was dead.\n",
      "\n",
      "Why don't lobsters share?     Because they are shellfish.\n",
      "\n",
      "Why does the man wish he could be a guitar player in a room full of beautiful girls?     Because if he was a guitar player, he would have his pick!\n",
      "\n",
      "Why did the ghost float across the road?     Because he couldn't walk.\n",
      "\n",
      "\n",
      "\n",
      "QUERY:why because HE\n",
      "RESULTS:\n",
      "Why did the elephant decide not to move?     Because he couldn't lift his trunk.\n",
      "\n",
      "Why is the little duck always so sad?     Because he always sees a bill in front of his face.\n",
      "\n",
      "Why does the man wish he could be a guitar player in a room full of beautiful girls?     Because if he was a guitar player, he would have his pick!\n",
      "\n",
      "Why did the ghost float across the road?     Because he couldn't walk.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Assignment 0\n",
    "\n",
    "You will implement a simple in-memory boolean search engine over the jokes\n",
    "from http://web.hawkesnest.net/~jthens/laffytaffy/.\n",
    "\n",
    "The documents are read from documents.txt.\n",
    "The queries to be processed are read from queries.txt.\n",
    "\n",
    "Your search engine will only need to support AND queries. A multi-word query\n",
    "is assumed to be an AND of the words. E.g., the query \"why because\" should be\n",
    "processed as \"why AND because.\"\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\" Convert a string representing one document into a list of\n",
    "    words. Remove all punctuation and split on whitespace.\n",
    "    Here is a doctest: \n",
    "    >>> tokenize(\"Hi  there. What's going on?\")\n",
    "    ['hi', 'there', 'what', 's', 'going', 'on']\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    temp_list1 = []\n",
    "    temp_1 = re.sub(r'[^\\w\\s]',' ',document).lower()\n",
    "    temp_list1 = temp_1.split()\n",
    "    \n",
    "    return temp_list1\n",
    "    \n",
    "\n",
    "def create_index(tokens):\n",
    "    \"\"\"\n",
    "    Create an inverted index given a list of document tokens. The index maps\n",
    "    each unique word to a list of document ids, sorted in increasing order.\n",
    "    Params:\n",
    "      tokens...A list of lists of strings\n",
    "    Returns:\n",
    "      An inverted index. This is a dict where keys are words and values are\n",
    "      lists of document indices, sorted in increasing order.\n",
    "    Below is an example, where the first document contains the tokens 'a' and\n",
    "    'b', and the second document contains the tokens 'a' and 'c'.\n",
    "    >>> index = create_index([['a', 'b'], ['a', 'c']])\n",
    "    >>> sorted(index.keys())\n",
    "    ['a', 'b', 'c']\n",
    "    >>> index['a']\n",
    "    [0, 1]\n",
    "    >>> index['b']\n",
    "    [0]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    index = {}\n",
    "    \n",
    "    for each_list in tokens:\n",
    "        for each_element in each_list:\n",
    "            if each_element not in index:\n",
    "                index[each_element] = []\n",
    "                index[each_element].append(tokens.index(each_list))\n",
    "            else:\n",
    "                index[each_element].append(tokens.index(each_list))\n",
    "                \n",
    "    for i in index:\n",
    "        index[i] = sorted(list(set(index[i])))\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def intersect(list1, list2):\n",
    "    \"\"\" Return the intersection of two posting lists. Use the optimize\n",
    "    algorithm of Figure 1.6 of the MRS text. Your implementation should be\n",
    "    linear in the sizes of list1 and list2. That is, you should only loop once\n",
    "    through each list.\n",
    "    Params:\n",
    "      list1....A list of document indices, sorted in ascending order.\n",
    "      list2....Another list of document indices, sorted in ascending order.\n",
    "    Returns:\n",
    "      The list of document ids that appear in both lists, sorted in ascending order.\n",
    "    >>> intersect([1, 3, 5], [3, 4, 5, 10])\n",
    "    [3, 5]\n",
    "    >>> intersect([1, 2], [3, 4])\n",
    "    []\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    i = 0\n",
    "    j = 0\n",
    "    temp_list2 = []\n",
    "    while i < len(list1) and j < (len(list2)):\n",
    "            if list1[i] == list2[j]:\n",
    "                temp_list2.append(list1[i])\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            elif list1[i] < list2[j]:\n",
    "                i = i + 1\n",
    "            elif list2[j] < list1[i]:\n",
    "                j = j + 1\n",
    "    return temp_list2\n",
    "\n",
    "\n",
    "def sort_by_num_postings(words, index):\n",
    "    \"\"\"\n",
    "    Sort the words in increasing order of the length of their postings list in\n",
    "    index. You may use Python's builtin sorted method.\n",
    "    Params:\n",
    "      words....a list of strings.\n",
    "      index....An inverted index; a dict mapping words to lists of document\n",
    "      ids, sorted in ascending order.\n",
    "    Returns:\n",
    "      A list of words, sorted in ascending order by the number of document ids\n",
    "      in the index.\n",
    "\n",
    "    >>> sort_by_num_postings(['a', 'b', 'c'], {'a': [0, 1], 'b': [1, 2, 3], 'c': [4]})\n",
    "    ['c', 'a', 'b']\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    dict_1 = {}\n",
    "    for i in words:\n",
    "        dict_1[i] = len(index[i])\n",
    "    return [x[0] for x in sorted(dict_1.items(), key=lambda x: x[1])]\n",
    "\n",
    "\n",
    "def search(index, query):\n",
    "    \"\"\" Return the document ids for documents matching the query. Assume that\n",
    "    query is a single string, possibly containing multiple words. The steps\n",
    "    are to:\n",
    "    1. tokenize the query\n",
    "    2. Sort the query words by the length of their postings list\n",
    "    3. Intersect the postings list of each word in the query.\n",
    "\n",
    "    If a query term is not in the index, then an empty list should be returned.\n",
    "\n",
    "    Params:\n",
    "      index...An inverted index (dict mapping words to document ids)\n",
    "      query...A string that may contain multiple search terms. We assume the\n",
    "      query is the AND of those terms by default.\n",
    "\n",
    "    E.g., below we search for documents containing 'a' and 'b':\n",
    "    >>> search({'a': [0, 1], 'b': [1, 2, 3], 'c': [4]}, 'a b')\n",
    "    [1]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    temp_list = []\n",
    "    tok = tokenize(query)\n",
    "    io = sort_by_num_postings(tok, index)\n",
    "    temp_list = index[io[0]]\n",
    "    \n",
    "    for i in io:\n",
    "            temp_list = intersect(temp_list, index[i])\n",
    "    \n",
    "    return temp_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main method. You should not modify this. \"\"\"\n",
    "    documents = open('documents.txt').readlines()\n",
    "    tokens = [tokenize(d) for d in documents]\n",
    "    index = create_index(tokens)\n",
    "    queries = open('queries.txt').readlines()\n",
    "    for query in queries:\n",
    "        results = search(index, query)\n",
    "        print('\\n\\nQUERY:%s\\nRESULTS:\\n%s' % (query, '\\n'.join(documents[r] for r in results)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
